{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF39Az6frznY",
        "outputId": "65c0317c-a529-4af7-de21-d90c41feb70e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ACHXx63Ivban"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/gdrive/MyDrive/flower_photos.zip'  # Update if in a folder\n",
        "extract_path = '/content/flower_photos'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e9ulEnntvvLC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di5zWhwpwoWd",
        "outputId": "355a8381-7c3d-4ce9-cfd5-75cd92642f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['flowers']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/flower_photos\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLQT6dhww75m",
        "outputId": "2580e42d-3d78-4dd1-e88c-ef63b33506ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-22 11:32:09--  https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.107.207, 192.178.163.207, 74.125.20.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.107.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228813984 (218M) [application/x-compressed-tar]\n",
            "Saving to: ‘flower_photos.tgz’\n",
            "\n",
            "flower_photos.tgz   100%[===================>] 218.21M  58.8MB/s    in 3.7s    \n",
            "\n",
            "2025-05-22 11:32:13 (58.8 MB/s) - ‘flower_photos.tgz’ saved [228813984/228813984]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/flower_photos\n",
        "!wget https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
        "!tar -xzf flower_photos.tgz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5j79vZcxA3M",
        "outputId": "963873a1-6bcc-43e1-d79f-55f4afe94eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sunflowers', 'daisy', 'LICENSE.txt', 'dandelion', 'tulips', 'roses']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/flower_photos\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQA2t8jJxroa",
        "outputId": "458cfc24-41ab-44f8-97f6-68e41d33afe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3670 files belonging to 5 classes.\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 2936 files for training.\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 734 files for validation.\n",
            "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
          ]
        }
      ],
      "source": [
        "num_classes = 5\n",
        "input_shape = (32,32,3)\n",
        "\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\"/content/flower_photos\")\n",
        "train = tf.keras.preprocessing.image_dataset_from_directory(\"/content/flower_photos\",\n",
        "                                                            validation_split=0.2,\n",
        "                                                            labels='inferred',\n",
        "                                                            subset=\"training\",\n",
        "                                                            image_size= (72,72),\n",
        "                                                            batch_size = 200,\n",
        "                                                            seed=123)\n",
        "validation = tf.keras.preprocessing.image_dataset_from_directory(\"/content/flower_photos\",\n",
        "                                                                 validation_split = 0.2,\n",
        "                                                                 labels=\"inferred\",\n",
        "                                                                 subset=\"validation\",\n",
        "                                                                 batch_size = 200,\n",
        "                                                                 image_size = (72,72),\n",
        "                                                                 seed=123)\n",
        "class_names = dataset.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9be2aEkWx6he",
        "outputId": "5fd3b878-acdc-4341-dd31-ee57cc35b147"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'train_iterator = train.as_numpy_iterator()\\ntrain_ds = train_iterator.next()\\nprint(train_ds[0].shape)\\nvalidation_iterator = validation.as_numpy_iterator()\\nvalidation_ds = validation_iterator.next()'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''train_iterator = train.as_numpy_iterator()\n",
        "train_ds = train_iterator.next()\n",
        "print(train_ds[0].shape)\n",
        "validation_iterator = validation.as_numpy_iterator()\n",
        "validation_ds = validation_iterator.next()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PmGGiHyoyATM"
      },
      "outputs": [],
      "source": [
        "image_size = 72\n",
        "preprocessingModel = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "augmentedModel = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
        "    tf.keras.layers.RandomFlip(\"vertical\"),\n",
        "    tf.keras.layers.RandomRotation(factor=0.02),\n",
        "    tf.keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4BxXqP_Nybpm"
      },
      "outputs": [],
      "source": [
        "train = train.map(lambda x,y:(preprocessingModel(x),y))\n",
        "validation = validation.map(lambda x,y:(preprocessingModel(x),y))\n",
        "train = train.map(lambda x,y:(augmentedModel(x),y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q-xDI_pSygSF"
      },
      "outputs": [],
      "source": [
        "train = train.prefetch(tf.data.AUTOTUNE)\n",
        "validation = validation.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NkXsWvtjylOt"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self , size , num_of_patches , projection_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.size=size\n",
        "    self.num_of_patches= num_of_patches + 1\n",
        "    self.projection_dim=projection_dim\n",
        "\n",
        "    self.projection=tf.keras.layers.Dense(projection_dim)\n",
        "\n",
        "    self.clsToken = self.add_weight(\n",
        "            name=\"clsToken\",\n",
        "            shape=(1, 1, projection_dim),\n",
        "            initializer=\"HeNormal\",  # Experiment with different initializers\n",
        "            trainable=True,\n",
        "        )\n",
        "    self.positionalEmbedding = tf.keras.layers.Embedding(self.num_of_patches , projection_dim)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    patches = tf.image.extract_patches(inputs , sizes=[1 , self.size , self.size , 1], strides=[1 , self.size , self.size , 1], rates=[1 ,1 ,1 ,1], padding=\"VALID\",)\n",
        "\n",
        "    patches=tf.reshape(patches, (tf.shape(inputs)[0], -1, self.size * self.size *3))\n",
        "    patches= self.projection(patches)\n",
        "\n",
        "    # repeat cls token length of batch size\n",
        "    clsToken = tf.repeat(self.clsToken , tf.shape(inputs)[0] , 0)\n",
        "    patches = tf.concat((clsToken, patches) , axis=1)\n",
        "    # create position number for each patch\n",
        "    positions = tf.range(0 , self.num_of_patches , 1)[tf.newaxis , ...]\n",
        "    positionalEmbedding = self.positionalEmbedding(positions)\n",
        "\n",
        "    #print(positionalEmbedding)\n",
        "    patches= patches + positionalEmbedding\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "o5Mbwz2yyp3k"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, heads, mlp_rate, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(heads, d_model//heads, dropout=dropout_rate)\n",
        "\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(d_model * mlp_rate, activation=\"gelu\"),  # Changed \"Relu\" to \"relu\"\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(d_model),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        out_1 = self.layernorm_1(inputs)\n",
        "        out_1 = self.mha(out_1, out_1, training=training)\n",
        "        out_1 = inputs + out_1\n",
        "\n",
        "        out_2 = self.layernorm_2(out_1)\n",
        "        out_2 = self.mlp(out_2, training=training)\n",
        "        out_2 = out_1 + out_2\n",
        "\n",
        "        out_3 = self.layernorm_3(out_2)\n",
        "        return out_3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6KeW8Qy4yvBY"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "  def __init__(self , d_model , heads , mlp_rate , num_layers=1 , dropout_rate=0.2):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoders = [TransformerLayer(d_model , heads , mlp_rate , dropout_rate) for _ in range(num_layers)]\n",
        "\n",
        "  def call(self , inputs , training=True):\n",
        "    x =inputs\n",
        "\n",
        "    for layer in self.encoders:\n",
        "      x = layer(x , training=training)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rU9wSDzay0Lo"
      },
      "outputs": [],
      "source": [
        "class ViT(tf.keras.Model):\n",
        "  def __init__(self , num_classes , patch_size , num_of_patches , d_model , heads , num_layers , mlp_rate , dropout_rate=0.2):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patchEmbedding = PatchEmbedding(patch_size , num_of_patches , d_model)\n",
        "    self.encoder = TransformerEncoder(d_model , heads , mlp_rate  ,num_layers , dropout_rate)\n",
        "    self.encoderNormalization = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.prediction = tf.keras.Sequential([\n",
        "                                           tf.keras.layers.Dropout(0.2),\n",
        "                                           tf.keras.layers.Dense(mlp_rate * d_model , activation=\"gelu\"),\n",
        "                                           tf.keras.layers.Dropout(0.2),\n",
        "                                           tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "\n",
        "  ])\n",
        "  def call(self , inputs ,  training=True):\n",
        "    patches = self.patchEmbedding(inputs) #patches will contain patch + positional information\n",
        "    encoderResult = self.encoder(patches, training=training)\n",
        "\n",
        "    clsResult = encoderResult[: , 0 , :]\n",
        "    clsResult = self.encoderNormalization(clsResult)\n",
        "    prediction = self.prediction(clsResult,\n",
        "                                 training=training)\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "igT71mphy74x"
      },
      "outputs": [],
      "source": [
        "def convert_to_dataset(data,batch_size,shuffle = False,augment = False):\n",
        "  dataset1 = data.map(lambda x,y:(preprocessingModel(x)[0],y),num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  if shuffle:\n",
        "    dataset1 = dataset1.shuffle(len(dataset1))\n",
        "\n",
        "  dataset1 = dataset1.batch(batch_size,drop_remainder = True)\n",
        "  if augment:\n",
        "    dataset1 = dataset1.map(lambda x,y:(augmentedModel(x,training = True),y),num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  return dataset1.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tFnUSjvazAZA"
      },
      "outputs": [],
      "source": [
        "vitClassifier = ViT(\n",
        "                5,\n",
        "                6,\n",
        "                (72//6)**2,\n",
        "                64,\n",
        "                5,\n",
        "                4,\n",
        "                3,\n",
        "                0.1\n",
        ")\n",
        "\n",
        "vitClassifier.compile(\n",
        "  optimizer=\"adam\",\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "  metrics=[\n",
        "      tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "      tf.keras.metrics.SparseTopKCategoricalAccuracy(10, name=\"top-10-accuracy\"),\n",
        "  ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCRmuzPczG30",
        "outputId": "eb2057b8-c6a1-43b3-ba7c-4dbf0fc0af7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 445ms/step - accuracy: 0.6315 - loss: 0.9275 - top-10-accuracy: 1.0000 - val_accuracy: 0.6281 - val_loss: 0.9712 - val_top-10-accuracy: 1.0000\n",
            "Epoch 2/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 569ms/step - accuracy: 0.6436 - loss: 0.9306 - top-10-accuracy: 1.0000 - val_accuracy: 0.6281 - val_loss: 0.9559 - val_top-10-accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 530ms/step - accuracy: 0.6438 - loss: 0.9264 - top-10-accuracy: 1.0000 - val_accuracy: 0.6730 - val_loss: 0.8714 - val_top-10-accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 488ms/step - accuracy: 0.6549 - loss: 0.8770 - top-10-accuracy: 1.0000 - val_accuracy: 0.6499 - val_loss: 0.9013 - val_top-10-accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 492ms/step - accuracy: 0.6795 - loss: 0.8422 - top-10-accuracy: 1.0000 - val_accuracy: 0.6866 - val_loss: 0.8301 - val_top-10-accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 552ms/step - accuracy: 0.6837 - loss: 0.8293 - top-10-accuracy: 1.0000 - val_accuracy: 0.6771 - val_loss: 0.8320 - val_top-10-accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 447ms/step - accuracy: 0.6845 - loss: 0.8202 - top-10-accuracy: 1.0000 - val_accuracy: 0.6703 - val_loss: 0.8233 - val_top-10-accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 462ms/step - accuracy: 0.6812 - loss: 0.8410 - top-10-accuracy: 1.0000 - val_accuracy: 0.6662 - val_loss: 0.9071 - val_top-10-accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 570ms/step - accuracy: 0.6850 - loss: 0.8521 - top-10-accuracy: 1.0000 - val_accuracy: 0.6853 - val_loss: 0.8337 - val_top-10-accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 487ms/step - accuracy: 0.6773 - loss: 0.8216 - top-10-accuracy: 1.0000 - val_accuracy: 0.6826 - val_loss: 0.8261 - val_top-10-accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 486ms/step - accuracy: 0.6904 - loss: 0.7818 - top-10-accuracy: 1.0000 - val_accuracy: 0.6866 - val_loss: 0.8064 - val_top-10-accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 551ms/step - accuracy: 0.7049 - loss: 0.7665 - top-10-accuracy: 1.0000 - val_accuracy: 0.6744 - val_loss: 0.8252 - val_top-10-accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 499ms/step - accuracy: 0.6806 - loss: 0.8046 - top-10-accuracy: 1.0000 - val_accuracy: 0.6975 - val_loss: 0.8082 - val_top-10-accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 444ms/step - accuracy: 0.7160 - loss: 0.7432 - top-10-accuracy: 1.0000 - val_accuracy: 0.6785 - val_loss: 0.8239 - val_top-10-accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 446ms/step - accuracy: 0.7013 - loss: 0.7590 - top-10-accuracy: 1.0000 - val_accuracy: 0.7044 - val_loss: 0.7687 - val_top-10-accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 539ms/step - accuracy: 0.7203 - loss: 0.7417 - top-10-accuracy: 1.0000 - val_accuracy: 0.6853 - val_loss: 0.8294 - val_top-10-accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 461ms/step - accuracy: 0.6964 - loss: 0.7828 - top-10-accuracy: 1.0000 - val_accuracy: 0.6921 - val_loss: 0.7816 - val_top-10-accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 485ms/step - accuracy: 0.6974 - loss: 0.7547 - top-10-accuracy: 1.0000 - val_accuracy: 0.6948 - val_loss: 0.7934 - val_top-10-accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 552ms/step - accuracy: 0.7262 - loss: 0.7513 - top-10-accuracy: 1.0000 - val_accuracy: 0.6880 - val_loss: 0.7971 - val_top-10-accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 490ms/step - accuracy: 0.7256 - loss: 0.7204 - top-10-accuracy: 1.0000 - val_accuracy: 0.6907 - val_loss: 0.7942 - val_top-10-accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "history = vitClassifier.fit(train,batch_size=200,validation_data=validation,epochs=20)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
